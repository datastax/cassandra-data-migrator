spark.source.isAstra                                    false
spark.source.host                                       localhost
spark.source.username                                   some-username
spark.source.password                                   some-secret-password
spark.source.read.consistency.level                     LOCAL_QUORUM
spark.source.keyspaceTable                              test.a1

spark.destination.isAstra                               true
spark.destination.scb                                   file:///aaa/bbb/secure-connect-enterprise.zip
spark.destination.username                              client-id
spark.destination.password                              client-secret
spark.destination.read.consistency.level                LOCAL_QUORUM
spark.destination.keyspaceTable                         test.a2
spark.destination.autocorrect.missing                   false
spark.destination.autocorrect.mismatch                  false

spark.maxRetries                                        10
spark.readRateLimit                                     20000
spark.writeRateLimit                                    20000
spark.splitSize                                         10000
spark.batchSize                                         5
spark.coveragePercent                                   100
spark.printStatsAfter                                   100000

spark.query.source                                      partition-key,clustering-key,order-date,amount,writetime(order-date),writetime(amount),ttl(order-date),ttl(amount)
spark.query.source.partitionKey                         partition-key
spark.query.destination                                 partition-key,clustering-key,order-date,amount
spark.query.destination.id                              partition-key,clustering-key
spark.query.types                                       9,1,4,3

spark.counterTable                                      false
spark.counterTable.cql
spark.counterTable.cql.index                            0

spark.preserveTTLWriteTime                              true
spark.source.ttl.cols                                   6,7

spark.source.writeTimeStampFilter                       false
spark.source.writeTimeStampFilter.cols                  4,5
spark.source.minWriteTimeStampFilter                    0
spark.source.maxWriteTimeStampFilter                    9223372036854775807

########################## ONLY USE if SSL clientAuth is enabled on source Cassandra/DSE ###############################
#spark.source.trustStore.path
#spark.source.trustStore.password
#spark.source.trustStore.type                            JKS
#spark.source.keyStore.path
#spark.source.keyStore.password
#spark.source.enabledAlgorithms                          TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

####################### ONLY USE if SSL clientAuth is enabled on destination Cassandra/DSE #############################
#spark.destination.trustStore.path
#spark.destination.trustStore.password
#spark.destination.trustStore.type                       JKS
#spark.destination.keyStore.path
#spark.destination.keyStore.password
#spark.destination.enabledAlgorithms                     TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

########################################################################################################################
# Following are the supported data types and their corresponding [Cassandra data-types]
# 0: String [ascii, text, varchar]
# 1: Integer [int, smallint]
# 2: Long [bigint, counter]
# 3: Double [double]
# 4: Instant [time, timestamp]
# 5: Map (separate type by %) [map] - Example: 5%1%0 for map<int, text>
# 6: List (separate type by %) [list] - Example: 6%0 for list<text>
# 7: ByteBuffer [blob]
# 8: Set (separate type by %) [set] - Example: 8%0 for set<text>
# 9: UUID [uuid, timeuuid]
# 10: Boolean [boolean]
# 11: TupleValue [tuple]
# 12: Float (float)
# 13: TinyInt [tinyint]
# 14: BigDecimal (decimal)
# 15: LocalDate (date)
# 16: UDT [any user-defined-type created using 'CREATE TYPE']
#
# Note:
# Enable "spark.preserveTTLWriteTime" only if you want to migrate writetimes and TTLs
#
# "spark.source.ttl.cols" - Comma separated column indexes from "spark.query.cols.select".
#  Script will only use the largest value per row.
#
# Include "writetime(column-name)" in "spark.query.cols.select" only if you want to use "writeTimeStampFilter" filter
#
# "spark.source.writeTimeStampFilter.cols" - Comma separated column indexes from "spark.query.cols.select".
#  Script will only use the largest value per row.
#
# Default value for "spark.source.maxWriteTimeStampFilter" is alway "9223372036854775807" (max long value)
#
# Properties "spark.query.cols.insert" and "spark.query.cols.insert.types" are required for "Migrate" job,
#  however they can be left empty for "DiffData" job
#
# Frozen has no impact on the mapping of Collections (Map/List/Set) - Example: 5%1%0 for frozen<map<int, text>>
#
########################################################################################################################
